{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":49330,"databundleVersionId":5211014,"sourceType":"competition"},{"sourceId":11650,"sourceType":"datasetVersion","datasetId":8327}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Disaster Tweet Classification using NLP and Deep Learning","metadata":{}},{"cell_type":"markdown","source":"### Project Description\n\n#### Overview\nThis project aims to develop a machine learning model capable of classifying tweets into two categories: those that are related to real disaster events and those that are not. Utilizing the \"Natural Language Processing with Disaster Tweets\" dataset from Kaggle, we employ NLP techniques and deep neural networks to analyze tweet text and predict their relevance to actual disasters.\n\n#### Dataset\nThe dataset is structured into two sets: training and testing data.\n\n- **Training Data**: Comprises 10,873 rows, including the header, with 5 columns:\n  - `id`: Unique identifier for each tweet.\n  - `text`: Text content of the tweet.\n  - `location`: Location from where the tweet was sent (may be blank).\n  - `keyword`: Specific keyword associated with the tweet (may be blank).\n  - `target`: Binary label indicating whether a tweet is about a real disaster (1) or not (0).\n\n- **Test Data**: Contains 10,875 rows, including the header, with the same columns as the training data except for the `target`.\n\n#### Objective\nThe primary goal is to preprocess the textual data effectively—handling its unstructured nature, slang, hashtags, and other idiosyncrasies—then apply and train a deep learning model to accurately classify tweets. Through this, we aim to demonstrate how NLP and deep learning techniques can be harnessed to distinguish between disaster-related communications and everyday conversations on social media.\n\n#### Approach\n- Preprocessing steps will include tokenization, removal of stop words, and stemming to prepare the text for modeling.\n- We plan to explore various vectorization techniques such as TF-IDF and word embeddings to convert text to numerical data.\n- For the classification model, we will investigate the use of recurrent neural networks (RNNs), particularly LSTM networks, and possibly Transformer-based models depending on the initial results.\n\nThis project not only serves as a practical application of NLP and deep learning but also contributes to the broader field of social media analytics, with potential implications for emergency response and disaster management.","metadata":{}},{"cell_type":"markdown","source":"### Exploratory Data Analysis (EDA) — Inspection, Visualization, and Cleaning\n\n**Objective:** Conduct an EDA to uncover patterns, anomalies, or inconsistencies, and prepare the dataset for modeling. This step is vital for understanding the data and informing our analysis strategy.\n\n#### Visualizations\n\n1. **Tweet Length Histogram**: Shows distribution of tweet lengths to identify verbosity patterns related to disaster and non-disaster tweets.\n2. **Keyword Frequency Bar Chart**: Displays top keywords for both categories, highlighting common disaster-related terms.\n3. **Location Distribution Visualization**: Indicates geographical trends in tweet origins, focusing on disaster-related tweets.\n\n#### Data Cleaning Procedures\n\n- **Missing Values**: Assess and handle missing 'location' and 'keyword' data, potentially filling in missing values or omitting these features based on their predictive value.\n- **Text Preprocessing**: Clean 'text' column by removing URLs, special characters, tokenization, removal of stop words, stemming, and standardizing text to reduce noise\n\n#### Analysis Plan Based on EDA\n\n- **Feature Engineering**: Create features reflecting tweet length and possibly derive sentiment scores or term frequency features from the text.\n- **Text Vectorization Choice**: Select a vectorization technique (TF-IDF or word embeddings) based on text data characteristics observed.\n- **Modeling Strategy**: Begin with a simple model for baseline performance, with plans to explore more sophisticated models like RNNs or Transformers influenced by EDA insights.\n- **Validation Approach**: Implement cross-validation, considering dataset balance and ensuring robust model evaluation.\n\nThis streamlined EDA will guide our cleaning, feature engineering, and modeling decisions, laying a solid foundation for tackling the classification challenge.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ninput_dir = '/kaggle/input/natural-language-processing-with-disaster-tweets'\n# Load the datasets\ntrain_df = pd.read_csv(f'{input_dir}/train.csv')\ntest_df = pd.read_csv(f'{input_dir}/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:29:42.070890Z","iopub.execute_input":"2024-04-11T22:29:42.071696Z","iopub.status.idle":"2024-04-11T22:29:44.298047Z","shell.execute_reply.started":"2024-04-11T22:29:42.071663Z","shell.execute_reply":"2024-04-11T22:29:44.297058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\n# Suppress FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Ensure there are no infinite values\ntrain_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# Drop rows with NaN values in the 'text' column if any exist (optional, based on your needs)\ntrain_df.dropna(subset=['text'], inplace=True)\n\n# Calculate tweet lengths\ntrain_df['tweet_length'] = train_df['text'].apply(len)\n\n# Plot histogram of tweet lengths using sns.histplot\nplt.figure(figsize=(10, 6))\nsns.histplot(train_df['tweet_length'], bins=30, kde=True)\nplt.title('Distribution of Tweet Lengths')\nplt.xlabel('Tweet Length')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:29:46.269327Z","iopub.execute_input":"2024-04-11T22:29:46.270308Z","iopub.status.idle":"2024-04-11T22:29:46.846175Z","shell.execute_reply.started":"2024-04-11T22:29:46.270268Z","shell.execute_reply":"2024-04-11T22:29:46.845182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate keyword\nkeyword_frequency = train_df[train_df['target'] == 1]['keyword'].value_counts().reset_index()\nkeyword_frequency.columns = ['keyword', 'frequency']\n\n# Sort by frequency\nkeyword_frequency = keyword_frequency.sort_values('frequency', ascending=False)\n\n# Plotting the Keyword Frequency Bar Chart ordered by frequency\nplt.figure(figsize=(12, 8))\nsns.barplot(y='keyword', x='frequency', data=keyword_frequency.head(20), order=keyword_frequency.head(20)['keyword'])\nplt.title('Top Keywords in Disaster Tweets by Frequency')\nplt.xlabel('Frequency')\nplt.ylabel('Keyword')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T22:34:21.309348Z","iopub.execute_input":"2024-04-08T22:34:21.310110Z","iopub.status.idle":"2024-04-08T22:34:21.764410Z","shell.execute_reply.started":"2024-04-08T22:34:21.310081Z","shell.execute_reply":"2024-04-08T22:34:21.763476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate location frequencies\nlocation_frequency = train_df[train_df['target'] == 1]['location'].value_counts().reset_index()\nlocation_frequency.columns = ['location', 'frequency']\n\n# Sort by frequency\nlocation_frequency = location_frequency.sort_values('frequency', ascending=False)\n\nplt.figure(figsize=(12, 8))\nsns.barplot(y='location', x='frequency', data=location_frequency.head(20), order=location_frequency.head(20)['location'])\nplt.title('Top Tweet Locations')\nplt.xlabel('Frequency')\nplt.ylabel('Location')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T22:37:27.195235Z","iopub.execute_input":"2024-04-08T22:37:27.195617Z","iopub.status.idle":"2024-04-08T22:37:27.642341Z","shell.execute_reply.started":"2024-04-08T22:37:27.195591Z","shell.execute_reply":"2024-04-08T22:37:27.641330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Cleaning\n\nTo enhance the quality of our dataset for classifying disaster-related tweets, we implemented crucial data cleaning steps. Missing entries in 'location' and 'keyword' columns were filled with 'unknown' to maintain dataset completeness. The text data underwent a thorough cleansing: converting to lowercase, removing URLs, user handles, and hashtags, discarding special characters, numbers, and non-ASCII characters. This process produced a clean and standardized text corpus, ensuring our model focuses on the meaningful linguistic patterns within tweets.","metadata":{}},{"cell_type":"code","source":"# Fill missing values with 'unknown'\ntrain_df['location'] = train_df['location'].fillna('unknown')\ntrain_df['keyword'] = train_df['keyword'].fillna('unknown')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:29:52.389949Z","iopub.execute_input":"2024-04-11T22:29:52.390338Z","iopub.status.idle":"2024-04-11T22:29:52.399153Z","shell.execute_reply.started":"2024-04-11T22:29:52.390293Z","shell.execute_reply":"2024-04-11T22:29:52.398125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# Define a function to clean the text\ndef clean_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Remove tweet handles\n    text = re.sub(r'@\\w+', '', text)\n    \n    # Remove only the hash symbol (#) from hashtags\n    text = re.sub(r'#', '', text)\n    \n    # Remove special characters and numbers\n    text = re.sub(r'\\W+|\\d+', ' ', text)\n    \n    # Remove non-ASCII characters\n    text = text.encode(\"ascii\", \"ignore\").decode()\n    \n    return text\n\n\n# Applying the cleaning function to your DataFrame\ntrain_df['cleaned_text'] = train_df['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:29:54.589971Z","iopub.execute_input":"2024-04-11T22:29:54.590354Z","iopub.status.idle":"2024-04-11T22:29:54.781692Z","shell.execute_reply.started":"2024-04-11T22:29:54.590320Z","shell.execute_reply":"2024-04-11T22:29:54.780706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Architecture Summary\n\n#### Vectorization Techniques\nOur approach to processing text involves two principal vectorization methods:\n\n- **GloVe Embeddings**: Leverages the GloVe model to produce dense representations of words by analyzing their co-occurrence patterns across large-scale text corpora, thereby enriching our model's understanding of linguistic subtleties.\n\n#### Classification Models\nTo address the challenge of classifying sequential text data, we plan to evaluate the following neural network architectures:\n\n- **LSTMs (Long Short-Term Memory Networks)**: Building upon the foundation of traditional RNNs, LSTMs are specifically engineered to capture and retain information across long sequences, showcasing their prowess in handling various text processing tasks. They are adept at circumventing the pitfalls commonly associated with standard RNNs, notably the vanishing gradient problem, thanks to their unique architecture that facilitates long-term memory.\n\n- **GRUs (Gated Recurrent Units)**: As a streamlined variant of LSTMs, GRUs simplify the architecture while maintaining the ability to manage information across extended sequences. They achieve this through a modified gating mechanism, which efficiently controls the flow of information without the complexity of LSTMs, making them a compelling option for tasks that require understanding temporal dynamics in data.\n\n#### Implementation Strategy\nWe'll start by exploring GloVe embeddings for our text representation, capitalizing on their rich pre-trained word vectors that capture deep lexical semantics based on global word-word co-occurrence statistics from a large corpus. Our initial modeling efforts will leverage LSTMs, renowned for their effectiveness in handling long-term dependencies in sequential text, to ascertain their suitability for our classification challenge. Should the need arise, we'll also consider GRUs for their efficiency and simplified architecture, which retains the core advantages of LSTMs but with potentially faster training times. This phased exploration of LSTM and GRU models, guided by performance metrics specific to disaster tweet classification, embodies our commitment to refining our approach to ensure superior model performance.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntweets = train_df['cleaned_text'].values\nlabels = train_df['target'].values\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:29:59.709140Z","iopub.execute_input":"2024-04-11T22:29:59.709993Z","iopub.status.idle":"2024-04-11T22:29:59.960683Z","shell.execute_reply.started":"2024-04-11T22:29:59.709963Z","shell.execute_reply":"2024-04-11T22:29:59.959836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import TextVectorization\n\nmax_features = 10000  # Vocabulary size\nmax_len = 140  # Sequence length to pad the sequences to\nembedding_dim = 300  # Dimensionality of the GloVe embeddings\n\n# Create the TextVectorization layer\nvectorize_layer = TextVectorization(\n    max_tokens=max_features,\n    output_mode='int',\n    output_sequence_length=max_len,\n    pad_to_max_tokens=True)\n\n# Fit the TextVectorization layer on the training data\nvectorize_layer.adapt(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:30:23.229162Z","iopub.execute_input":"2024-04-11T22:30:23.229516Z","iopub.status.idle":"2024-04-11T22:30:23.296518Z","shell.execute_reply.started":"2024-04-11T22:30:23.229491Z","shell.execute_reply":"2024-04-11T22:30:23.295621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\nerror_count = 0\n\nwith open('/kaggle/input/glove840b300dtxt/glove.840B.300d.txt', 'r', encoding='utf-8') as f:\n    for line in f:\n        parts = line.rstrip().split(' ')\n        word = parts[0]\n        try:\n            coefs = np.asarray(parts[1:], dtype='float32')\n            embeddings_index[word] = coefs\n        except ValueError:\n            error_count += 1\n            print(f\"Error converting {word}'s embeddings; skipped.\")\n\nprint(f\"Finished parsing embeddings. Number of lines with errors: {error_count}\")\n\n# Prepare the GloVe embedding matrix\nvocab = vectorize_layer.get_vocabulary()\nembedding_matrix = np.zeros((max_features, embedding_dim))\nfor i, word in enumerate(vocab):\n    if i < max_features:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:30:26.794095Z","iopub.execute_input":"2024-04-11T22:30:26.794959Z","iopub.status.idle":"2024-04-11T22:33:40.245025Z","shell.execute_reply.started":"2024-04-11T22:30:26.794928Z","shell.execute_reply":"2024-04-11T22:33:40.244106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initial LSTM Model and Training\n\nWe will start by setting up an initial model using a RNN with an LSTM layer and then train it with a reasonable amount of epochs to see how the validation accuracy responds over time.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.initializers import Constant\n\n# Define the model with the TextVectorization layer first\nmodel = Sequential([\n    vectorize_layer,  # TextVectorization layer\n    Embedding(\n        input_dim=max_features,\n        output_dim=embedding_dim,\n        embeddings_initializer=Constant(embedding_matrix),\n        trainable=False),  # GloVe Embedding layer\n    Bidirectional(LSTM(64, return_sequences=True)),\n    Bidirectional(LSTM(32)),\n    Dense(64, activation='relu'),\n    Dense(1, activation='sigmoid')  # Assuming binary classification\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:34:03.964808Z","iopub.execute_input":"2024-04-11T22:34:03.965573Z","iopub.status.idle":"2024-04-11T22:34:04.033196Z","shell.execute_reply.started":"2024-04-11T22:34:03.965514Z","shell.execute_reply":"2024-04-11T22:34:04.032204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    X_train,\n    y_train,\n    validation_data=(X_test, y_test),\n    batch_size=32,\n    epochs=10,\n    verbose=1,\n)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T03:29:28.745340Z","iopub.execute_input":"2024-04-10T03:29:28.745686Z","iopub.status.idle":"2024-04-10T03:30:40.427217Z","shell.execute_reply.started":"2024-04-10T03:29:28.745658Z","shell.execute_reply":"2024-04-10T03:30:40.426357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initial Results and Model Changes\n\nThe initial results using LSTM were promising. One thing thing that stood out was the validation accuracy peaking after two epochs. This is likely due to overfitting of the training data to the model. In order to mitigate this issue dropout layers can be added, which will help with overfitting by randomly dropping out layers in the previous layer of the neural network. A regulization term can also be added to the Dense layer in the model which will penalize weights that are too large and thus lessen the models ability to fit the training data too tightly.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.regularizers import l2\n\nmodel = Sequential([\n    vectorize_layer,\n     Embedding(\n        input_dim=max_features,\n        output_dim=embedding_dim,\n        embeddings_initializer=Constant(embedding_matrix),\n        trainable=False),\n    Bidirectional(LSTM(64, return_sequences=True)),\n    Dropout(0.5),  # Adding dropout\n    Bidirectional(LSTM(32)),\n    Dropout(0.5),  # Adding another dropout layer\n    Dense(64, activation='relu', kernel_regularizer=l2(0.05)),\n    Dropout(0.5),  # Adding dropout before the final layer\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:43:44.331205Z","iopub.execute_input":"2024-04-11T22:43:44.332091Z","iopub.status.idle":"2024-04-11T22:43:44.374215Z","shell.execute_reply.started":"2024-04-11T22:43:44.332049Z","shell.execute_reply":"2024-04-11T22:43:44.373502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    X_train,\n    y_train,\n    validation_data=(X_test, y_test),\n    batch_size=32,\n    epochs=10,\n    verbose=1,\n)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T03:30:54.442276Z","iopub.execute_input":"2024-04-10T03:30:54.443124Z","iopub.status.idle":"2024-04-10T03:32:04.385165Z","shell.execute_reply.started":"2024-04-10T03:30:54.443094Z","shell.execute_reply":"2024-04-10T03:32:04.384244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Overfitting Mitigation Results and Hyperparameter Optimization\n\nThe result was not quite what was expected, though they are still an improvement over the previous model. The model's validation accuracy still peaked after the second epoch, but the validation accuracy increased and therefore the overfitting mitigation midifications to the model will be used in subsequent models.\n\nThe next step in the model building process is to perform hyperparameter optimization. The `keras_tuner` library will be used to test different combinations of hyperparameter values and iterate over the results in order to find the optimal model. In order to do this a new class must be created that will allow for the testing the hyperparameters.","metadata":{}},{"cell_type":"code","source":"from keras_tuner import HyperModel\n\nclass LSTMHyperModel(HyperModel):\n\n    def __init__(self, vectorize_layer, max_features):\n        self.vectorize_layer = vectorize_layer\n        self.max_features = max_features\n\n    def build(self, hp):\n        model = Sequential([\n            self.vectorize_layer,\n             Embedding(\n                input_dim=max_features,\n                output_dim=embedding_dim,\n                embeddings_initializer=Constant(embedding_matrix),\n                trainable=False),\n            Bidirectional(LSTM(units=hp.Int('lstm_units_1', min_value=32, max_value=128, step=32), return_sequences=True)),\n            Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.7, step=0.1)),\n            Bidirectional(LSTM(units=hp.Int('lstm_units_2', min_value=16, max_value=64, step=16))),\n            Dropout(hp.Float('dropout_2', min_value=0.3, max_value=0.7, step=0.1)),\n            Dense(\n                units=hp.Int('dense_units', min_value=32, max_value=128, step=32), \n                activation='relu',\n                kernel_regularizer=l2(hp.Float('l2_regularization', min_value=1e-5, max_value=1e-2, sampling='LOG'))\n            ),\n            Dropout(hp.Float('dropout_3', min_value=0.3, max_value=0.7, step=0.1)),\n            Dense(1, activation='sigmoid')\n        ])\n\n        model.compile(optimizer='adam',\n                      loss='binary_crossentropy',\n                      metrics=['accuracy'])\n        \n        return model","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:34:55.995521Z","iopub.execute_input":"2024-04-11T22:34:55.996221Z","iopub.status.idle":"2024-04-11T22:34:56.006241Z","shell.execute_reply.started":"2024-04-11T22:34:55.996191Z","shell.execute_reply":"2024-04-11T22:34:56.005176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_tuner.tuners import RandomSearch\n\nhypermodel = LSTMHyperModel(vectorize_layer=vectorize_layer, max_features=max_features)\n\ntuner = RandomSearch(\n    hypermodel,\n    objective='val_accuracy',\n    max_trials=30,  # Adjust based on computational resources\n    executions_per_trial=1,\n    project_name='disaster_tweet_classification_ltsm'\n)\n\ntuner.search(tweets, labels, epochs=5, validation_split=0.2)  # Adjust epochs based on needs\n\n# Fetch the best model\nlstm_best_model = tuner.get_best_models(num_models=1)[0]\nlstm_best_model.summary()\n\n# Evaluate the best model\nloss, accuracy = lstm_best_model.evaluate(X_test, y_test)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:35:00.454291Z","iopub.execute_input":"2024-04-11T22:35:00.454924Z","iopub.status.idle":"2024-04-11T22:35:05.813188Z","shell.execute_reply.started":"2024-04-11T22:35:00.454892Z","shell.execute_reply":"2024-04-11T22:35:05.812378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Optimization Results\n\nThe hyperparamter optimization was successful in raising the validation accuracy by about four percent. The summary of the model can be seen above and the weights and hyperparameters used in the most accurate model will be used for the final evaluation of the model.","metadata":{}},{"cell_type":"markdown","source":"### Initial GRU Model and Training\n\nA model similar to the LSTM RNN will be used except with GRU replacing the LSTM layers. It will also carry over the same overfitting mitigation layers to help prevent overfitting.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import GRU\n\nmodel = Sequential([\n    vectorize_layer,  # TextVectorization layer for preprocessing\n     Embedding(\n        input_dim=max_features,\n        output_dim=embedding_dim,\n        embeddings_initializer=Constant(embedding_matrix),\n        trainable=False),\n    Bidirectional(GRU(64, return_sequences=True)),\n    Dropout(0.5),\n    Bidirectional(GRU(32)),\n    Dropout(0.5),\n    Dense(64, activation='relu', kernel_regularizer=l2(0.05)),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:35:19.100082Z","iopub.execute_input":"2024-04-11T22:35:19.100447Z","iopub.status.idle":"2024-04-11T22:35:19.144135Z","shell.execute_reply.started":"2024-04-11T22:35:19.100420Z","shell.execute_reply":"2024-04-11T22:35:19.143205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    X_train,\n    y_train,\n    validation_data=(X_test, y_test),\n    batch_size=32,\n    epochs=10,\n    verbose=1,\n)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:23:54.723921Z","iopub.execute_input":"2024-04-10T05:23:54.724280Z","iopub.status.idle":"2024-04-10T05:25:05.384918Z","shell.execute_reply.started":"2024-04-10T05:23:54.724250Z","shell.execute_reply":"2024-04-10T05:25:05.384007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initial Results and Hyperparameter Tuning\n\nThe initial results are promising with a slightly increased validation accuracy compared to a similar LSTM model. To continue optimization of the model hyperparameter tuning will be done in a similar manner to the LSTM hyperparamer tuning.","metadata":{}},{"cell_type":"code","source":"class GRUHyperModel(HyperModel):\n\n    def __init__(self, vectorize_layer, max_features):\n        self.vectorize_layer = vectorize_layer\n        self.max_features = max_features\n\n    def build(self, hp):\n        model = Sequential([\n            self.vectorize_layer,\n             Embedding(\n                input_dim=max_features,\n                output_dim=embedding_dim,\n                embeddings_initializer=Constant(embedding_matrix),\n                trainable=False),\n            Bidirectional(GRU(units=hp.Int('gru_units_1', min_value=32, max_value=128, step=32), return_sequences=True)),\n            Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.7, step=0.1)),\n            Bidirectional(GRU(units=hp.Int('gru_units_2', min_value=16, max_value=64, step=16))),\n            Dropout(hp.Float('dropout_2', min_value=0.3, max_value=0.7, step=0.1)),\n            Dense(\n                units=hp.Int('dense_units', min_value=32, max_value=128, step=32), \n                activation='relu',\n                kernel_regularizer=l2(hp.Float('l2_regularization', min_value=1e-5, max_value=1e-2, sampling='LOG'))\n            ),\n            Dropout(hp.Float('dropout_3', min_value=0.3, max_value=0.7, step=0.1)),\n            Dense(1, activation='sigmoid')\n        ])\n\n        model.compile(optimizer='adam',\n                      loss='binary_crossentropy',\n                      metrics=['accuracy'])\n        \n        return model","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:35:22.319960Z","iopub.execute_input":"2024-04-11T22:35:22.320438Z","iopub.status.idle":"2024-04-11T22:35:22.330996Z","shell.execute_reply.started":"2024-04-11T22:35:22.320403Z","shell.execute_reply":"2024-04-11T22:35:22.329892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hypermodel = GRUHyperModel(vectorize_layer=vectorize_layer, max_features=max_features)\n\ntuner = RandomSearch(\n    hypermodel,\n    objective='val_accuracy',\n    max_trials=30,  # Adjust based on computational resources\n    executions_per_trial=1,\n    project_name='disaster_tweet_classification_gru'\n)\n\ntuner.search(tweets, labels, epochs=5, validation_split=0.2)  # Adjust epochs based on needs\n\n# Fetch the best model\ngru_best_model = tuner.get_best_models(num_models=1)[0]\ngru_best_model.summary()\n\n# Evaluate the best model\nloss, accuracy = gru_best_model.evaluate(X_test, y_test)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:35:25.169572Z","iopub.execute_input":"2024-04-11T22:35:25.169960Z","iopub.status.idle":"2024-04-11T22:35:27.212578Z","shell.execute_reply.started":"2024-04-11T22:35:25.169931Z","shell.execute_reply":"2024-04-11T22:35:27.211295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Tuning Results\n\nAgain the hyperparameter tuning was able to significantly improve the performance of the model. The validation accuracy was increased by four percent and the optimal weights and hyperparameters will be used for the final evaluation of the GRU model.","metadata":{}},{"cell_type":"markdown","source":"### Final Evaluation\n\nTo get a detailed overview of how each model is performing a combination of a confusion matrix, ROC curve, and F1 score will be used to help make a decision on which model should be used for the final predictions on the test dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, f1_score\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve, auc\n\ndef display_confusion_matrix(y_pred, y):\n    y_pred = (y_pred > 0.5).astype(int)  # Threshold the probabilities to get binary class predictions\n    \n    conf_matrix = confusion_matrix(y, y_pred)\n    plt.figure(figsize=(10,7))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\")\n    plt.xlabel('Predicted labels')\n    plt.ylabel('True labels')\n    plt.title('Confusion Matrix')\n    plt.show()\n    \ndef display_roc_curve(y_pred, y):\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(fpr, tpr)\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \ndef display_f1_score(y_pred, y):\n    y_pred = (y_pred > 0.5).astype(int)  # Threshold at 0.5\n    f1 = f1_score(y_test, y_pred, average='binary')\n    print(f'F1 Score: {f1}')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:55:37.509744Z","iopub.execute_input":"2024-04-11T22:55:37.510402Z","iopub.status.idle":"2024-04-11T22:55:37.521671Z","shell.execute_reply.started":"2024-04-11T22:55:37.510371Z","shell.execute_reply":"2024-04-11T22:55:37.520672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM Final Evaluation","metadata":{}},{"cell_type":"code","source":"y_pred = lstm_best_model.predict(X_test)\ndisplay_f1_score(y_pred, y_test)\ndisplay_roc_curve(y_pred, y_test)\ndisplay_confusion_matrix(y_pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T22:40:31.330186Z","iopub.execute_input":"2024-04-11T22:40:31.330593Z","iopub.status.idle":"2024-04-11T22:40:32.511958Z","shell.execute_reply.started":"2024-04-11T22:40:31.330563Z","shell.execute_reply":"2024-04-11T22:40:32.511020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GRU Final Evaluation","metadata":{}},{"cell_type":"code","source":"y_pred = gru_best_model.predict(X_test)\ndisplay_f1_score(y_pred, y_test)\ndisplay_roc_curve(y_pred, y_test)\ndisplay_confusion_matrix(y_pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T23:01:03.964819Z","iopub.execute_input":"2024-04-11T23:01:03.965382Z","iopub.status.idle":"2024-04-11T23:01:05.150835Z","shell.execute_reply.started":"2024-04-11T23:01:03.965333Z","shell.execute_reply":"2024-04-11T23:01:05.149896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nIn this project, we explored the utility of Recurrent Neural Network (RNN) architectures, specifically Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) models, for the classification of tweets into two categories: those related to natural disasters and those that are not. The robustness and suitability of these models were rigorously evaluated through performance metrics and visual analysis tools like ROC curves and confusion matrices.\n\nThe LSTM model demonstrated a commendable F1 score of 0.8319604612850082, suggesting a strong balance between precision and recall in classification tasks. This high F1 score is indicative of the LSTM's capability to model long-term dependencies, reflecting its architectural strengths in managing sequences with intricate structures—a common characteristic in language data.\n\nThe GRU model, with a slightly lower F1 score of 0.8228176318063958, also performed admirably, proving the effectiveness of its simpler structure, which can often yield faster training times and require less computational resources.\n\nUpon analyzing the Receiver Operating Characteristic (ROC) curves, both models showcased excellent discriminative ability with an area under the curve (AUC) of 0.92 for each. This is a testament to their capability to distinguish between disaster-related and non-disaster tweets with high confidence.\n\nThe decision matrices for both models further complemented the ROC analysis, providing insight into the true positive and false positive rates that informed the ROC curves' shape.\n\nWhen comparing the two models, the LSTM showed a slight edge in F1 score, which often translates to better performance on balanced datasets. Given the closeness in AUC values, the higher F1 score becomes a deciding factor, leading us to favor the LSTM model for making predictions on test data. This choice is backed by the LSTM's intrinsic ability to capture and utilize the context over longer spans of text, which can be crucial for the nuances involved in classifying disaster-related content accurately.\n\nIn conclusion, the LSTM model is selected as the preferred model for deployment in predicting whether tweets signify natural disasters. Its proven performance, supported by both numerical metrics and visual evaluations, positions it as the more robust model for understanding the complexity of natural language within the scope of our classification task.","metadata":{}},{"cell_type":"code","source":"test_df['location'] = test_df['location'].fillna('unknown')\ntest_df['keyword'] = test_df['keyword'].fillna('unknown')\ntest_df['cleaned_text'] = test_df['text'].apply(clean_text)\n\npredictions = lstm_best_model.predict(test_df['cleaned_text'].values)\npredictions = (predictions > 0.5).astype(int)  # Apply threshold for binary classification\n\nsubmission_df = pd.DataFrame(test_df['id'])\nsubmission_df['target'] = predictions\n\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T23:12:14.755324Z","iopub.execute_input":"2024-04-11T23:12:14.756004Z","iopub.status.idle":"2024-04-11T23:12:16.084977Z","shell.execute_reply.started":"2024-04-11T23:12:14.755971Z","shell.execute_reply":"2024-04-11T23:12:16.083992Z"},"trusted":true},"execution_count":null,"outputs":[]}]}